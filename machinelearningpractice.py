# -*- coding: utf-8 -*-
"""MachineLearningPractice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tQNaVObU-K9SwY4JLmmdCKBXDO2YmWRa

# Regression Intro
"""

import sklearn
!pip install quandl
import quandl
import pandas as pd

df = quandl.get("EOD/MSFT", authtoken="3xHuLJxRdkGeukHygJ9x")

df = df[['Adj_Open', 'Adj_High','Adj_Low','Adj_Close','Adj_Volume']]
df['HL_PCT'] = (df['Adj_High']- df['Adj_Close']) / df['Adj_Close'] * 100.0
df['PCT_change'] = (df['Adj_Close']- df['Adj_Open']) / df['Adj_Open'] * 100.0

df = df[['Adj_Close', 'HL_PCT', 'PCT_change', 'Adj_Volume']]
print(df)

"""Regression Features and Labels"""

import math
forecast_col = 'Adj_Close'
df.fillna(value=-99999, inplace = True) #the 99999 is treated as an outlier

forecast_out = int(math.ceil(0.01*len(df))) #turn into integer, 
                            #predicting 10% of dataframmath.ceil rounds up
print(forecast_out)
df['label'] = df[forecast_col].shift(-forecast_out) #shifting 'spreadsheet up'
print(df) #to predict 10% out (10 days?) label is the predicted price in 10 percent

"""Regression Training and Testing

Forcasting and Predicting

Pickling is serialization
"""

import numpy as np
import sklearn
from sklearn import preprocessing
from sklearn import svm
from sklearn.linear_model import LinearRegression
#from sklearn import cross_validation
from sklearn.model_selection import train_test_split
import matplotlib
from matplotlib import pyplot as plt
from matplotlib import style
import datetime
import pickle

style.use('ggplot')

X = np.array(df.drop(['label'], 1)) #returns new data frame as array
x = preprocessing.scale(X) 
X=X[:-forecast_out]
X_lately = X[-forecast_out:] #what we are predicting aginst, don't have y


df.dropna(inplace=True)
y = np.array(df['label'])
y = np.array(df['label'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#Regression and line and fit
"""clf = LinearRegression(n_jobs=-1)

clf.fit(X_train, y_train)

#save the classifier to avoid the training step
with open('linearregression.pickle', 'wb') as f:
  pickle.dump(clf, f)"""

pickle_in = open('linearregression.pickle', 'rb') 
clf = pickle.load(pickle_in)  
accuracy = clf.score(X_test, y_test) #pickle is saved as file

forecast_set = clf.predict(X_lately) #put in arrays of what you want to predict
#clf.predict(x_lately) IS THE CRUX OF THE PREDICTION
#each forecast is just one day later

print(forecast_set, accuracy, forecast_out)
df['Forecast'] = np.nan
#x is the features, y is the labels NOT necessarily axis
last_date = df.iloc[-1].name
last_unix = last_date.timestamp()
one_day = 86400
next_unix = last_unix + one_day

#below is just to make sure there are dates on the axis
for i in forecast_set:
  next_date = datetime.datetime.fromtimestamp(next_unix)
  next_unix += one_day
  df.loc[next_date] = [np.nan for _ in range(len(df.columns)-1)] + [i]

df['Adj_Close'].plot()
df['Forecast'].plot()
plt.legend(loc=4)
plt.xlabel('Date')
plt.ylabel('Price')
plt.show()

clf = svm.SVR(kernel='poly')

clf.fit(X_train, y_train)

accuracy = clf.score(X_test, y_test)
print(accuracy) #comes out very innacurate, 'poly' gets negative acc

"""Linear Regression From Scratch: How to find the Best Fit Slope"""

from statistics import mean
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
import random

style.use('fivethirtyeight')

#xs = np.array([1, 2, 3, 4, 5, 6], dtype=np.float64)
y#s = np.array([5, 4, 6, 5, 6, 7], dtype=np.float64)

#testing assumptions
def create_dataset(hm, variance, step=2, correlation=False):
  val = 1
  ys = []
  for i in range(hm):
    y= val + random.randrange(-variance, variance)
    ys.append(y)
    if correlation and correlation == 'pos':
      val +=step
    elif correlation and correlation == 'neg':
      val -= step
  xs = [i for i in range(len(ys)) ]

  return np.array(xs, dtype=np.float64), np.array(ys, dtype=np.float64)

def best_fit_slope_and_intercept(xs, ys):
  m = (  ((mean(xs) * mean(ys)) - mean(xs*ys))  /
      ( (mean(xs)**2) - mean((xs)**2)  )  )
  b = mean(ys) - m*mean(xs)
  return m, b

xs, ys = create_dataset(40, 80, 2, correlation=False) 

m, b = best_fit_slope_and_intercept(xs, ys) 
#print(m, b) 

regression_line = [(m*x) + b for x in xs]

predict_x = 8
predict_y = (m*predict_x)+b

plt.scatter(xs, ys)
plt.scatter(predict_x, predict_y, s=80, color='g')
plt.plot(xs, regression_line)
plt.show

def squared_error(ys_orig, ys_line):
  return sum((ys_line-ys_orig)**2)

def coefficient_of_determination(ys_orig, ys_line):
  y_mean_line = [mean(ys_orig) for y in ys_orig]
  squared_error_regr =   squared_error(ys_orig, ys_line)
  squared_error_y_mean = squared_error(ys_orig, y_mean_line)
  return 1 -(squared_error_regr / squared_error_y_mean)

r_squared = coefficient_of_determination(ys, regression_line)
print(r_squared)

"""# K Nearest Neighbors Classification"""

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import sklearn
from sklearn import preprocessing, neighbors
from sklearn.model_selection import train_test_split
import pandas as pd

df = pd.read_csv('breast-cancer-wisconsin.data')
df.replace('?', -99999, inplace = True)
df.drop(['id'], 1, inplace=True)

X = np.array(df.drop(['class'], 1))
y = np.array(df['class'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf = neighbors.KNeighborsClassifier()
clf.fit(X_train, y_train)

accuracy = clf.score(X_test, y_test)

print(accuracy)

example_measures = np.array([4,2,1,1,1,2,3,2,1]) #this set does not exist

example_measures = example_measures.reshape(1, -1)

prediction = clf.predict(example_measures)

print(prediction)
#2 is benign

from math import sqrt
import numpy as np
import matplotlib.pyplot as plt
import warnings
from matplotlib import style
from collections import Counter
style.use('fivethirtyeight')

dataset = {'k': [[1,2],[2,3],[3,1]], 'r':[[6,5], [7,7], [8,6]]}
new_features = [5, 7]

[[plt.scatter(ii[0], ii[1], s=100, color = i) for ii in dataset[i]] for i in dataset]
plt.scatter(new_features[0], new_features[1])
plt.show()

def k_nearest_neighbors(data, predict, k=3):
  if len(data) >= k:
    warnings.warn('K is set to a value less than total voting groups!')
  distances = []
  for group in data:
    for features in data[group]:
      euclidean_distance = np.linalg.norm(np.array(features)-np.array(predict))
      distances.append([euclidean_distance, group])
  votes = [i[1] for i in sorted(distances)[:k]]
  print(Counter(votes).most_common(1))
  vote_result = Counter(votes).most_common(1)[0][0]  

  return vote_result  

result = k_nearest_neighbors(dataset, new_features, k=3)
print(result)

[[plt.scatter(ii[0], ii[1], s=100, color = i) for ii in dataset[i]] for i in dataset]
plt.scatter(new_features[0], new_features[1])
plt.show()

from math import sqrt
import numpy as np
import warnings
from collections import Counter
import random
import pandas as pd

def k_nearest_neighbors(data, predict, k=3):
  if len(data) >= k:
    warnings.warn('K is set to a value less than total voting groups!')
  distances = []
  for group in data:
    for features in data[group]:
      euclidean_distance = np.linalg.norm(np.array(features)-np.array(predict))
      distances.append([euclidean_distance, group])
  votes = [i[1] for i in sorted(distances)[:k]]
  #print(Counter(votes).most_common(1))
  vote_result = Counter(votes).most_common(1)[0][0]  

  return vote_result  

df = pd.read_csv('breast-cancer-wisconsin.data')
df.replace('?', -99999, inplace = True)
df.drop(['id'], 1, inplace=True)
full_data = df.astype(float).values.tolist() #makes sure to convert to float

random.shuffle(full_data)

test_size = 0.2
train_set = {2:[],4:[]}
test_set = {2:[],4:[]}
train_data = full_data[:-int(test_size*len(full_data))] #first 20 percent
test_data = full_data[-int(test_size*len(full_data)):]  #last 20 percent

for i in train_data:
  train_set[i[-1]].append(i[:-1])

for i in test_data:
  test_set[i[-1]].append(i[:-1])

correct = 0 
total =  0
for group in test_set:
  for data in test_set[group]:
    vote  = k_nearest_neighbors(train_set, data, k=5)
    if group == vote:
      correct += 1
    total += 1  

print('Accuracy:', correct/total)

"""Video 19 Confidence and SKLearn"""

from math import sqrt
import numpy as np
import warnings
from collections import Counter
import random
import pandas as pd

def k_nearest_neighbors(data, predict, k=3):
  if len(data) >= k:
    warnings.warn('K is set to a value less than total voting groups!')
  distances = []
  for group in data:
    for features in data[group]:
      euclidean_distance = np.linalg.norm(np.array(features)-np.array(predict))
      distances.append([euclidean_distance, group])
  votes = [i[1] for i in sorted(distances)[:k]]
  #print(Counter(votes).most_common(1))
  vote_result = Counter(votes).most_common(1)[0][0] 
#######################################################################   
###THIS IS WHERE CONFIDENCE COMES IN
  confidence = Counter(votes).most_common(1)[0][1] / k
  
  #print(vote_result, confidence)
  
  return vote_result, confidence  

accuracies = []

for i in range(25):
  df = pd.read_csv('breast-cancer-wisconsin.data')
  df.replace('?', -99999, inplace = True)
  df.drop(['id'], 1, inplace=True)
  full_data = df.astype(float).values.tolist() #makes sure to convert to float

  random.shuffle(full_data)

  test_size = 0.4
  train_set = {2:[],4:[]}
  test_set = {2:[],4:[]}
  train_data = full_data[:-int(test_size*len(full_data))] #first 20 percent
  test_data = full_data[-int(test_size*len(full_data)):]  #last 20 percent

  for i in train_data:
    train_set[i[-1]].append(i[:-1])

  for i in test_data:
    test_set[i[-1]].append(i[:-1])

  correct = 0 
  total =  0

  for group in test_set:
    for data in test_set[group]:
      vote, confidence = k_nearest_neighbors(train_set, data, k=5)
      if group == vote:
        correct += 1
      #else:
        #print(confidence)  
      total += 1  

  print('Accuracy:', correct/total) 
  accuracies.append(correct/total)

print(sum(accuracies)/len(accuracies))

"""# Support Vector Machine

Video 20
"""

import numpy as np
import sklearn
from sklearn import preprocessing, neighbors
from sklearn.model_selection import train_test_split
from sklearn import svm
import pandas as pd

df = pd.read_csv('breast-cancer-wisconsin.data')
df.replace('?', 0, inplace = True)
df.drop(['id'], 1, inplace=True)

X = np.array(df.drop(['class'], 1))
y = np.array(df['class'])
print(df.columns)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf = svm.SVC()
clf.fit(X_train, y_train)

accuracy = clf.score(X_test, y_test)

print(accuracy)

#example_measures = np.array([[4,2,1,1,1,2,3,2,1], [4,2,1,2,2,2,3,2,1]] ) #this set does not exist

#example_measures = example_measures.reshape(len(example_measures), -1)

#prediction = clf.predict(example_measures)

#print(prediction) #doesn't seem to come out with right output???

"""Video 22-28

SVM from scratch
"""

import matplotlib.pyplot as plt
from matplotlib import style
import numpy as np
style.use('ggplot')

class Support_Vector_Machine:
  def __init__(self, visualization = True):
    self.visualization = visualization
    self.colors = {1:'r', -1:'b'}
    if self.visualization:
      self.fig = plt.figure()
      self.ax = self.fig.add_subplot(1,1,1)

  #train
  def fit(self, data):
    self.data = data
    # { ||w||: [w,b]  } 
    opt_dict = {}

    transforms=[[1,1],
                [-1,1],
                [-1, -1],
                [1, -1]]

    all_data = []
    for yi in self.data:
      for featureset in self.data[yi]:
        for feature in featureset:
          all_data.append(feature)

    self.max_feature_value = max(all_data)
    self.min_feature_value = min(all_data)                          
    all_data = None

    # support vectors yi (xi.w +b) = 1

    step_sizes = [self.max_feature_value * 0.1, #step sizes
                  self.max_feature_value * 0.01,
                  #point of expense: 
                  self.max_feature_value * 0.001]

    #extremely expensive
    b_range_multiple = 5

    # we dont need to take as small of steps w b as we do w 'w'
    b_multiple = 5
    latest_optimum = self.max_feature_value*10

    for step in step_sizes:
      w = np.array([latest_optimum, latest_optimum])
      
      #we can do this because convex
      optimized = False
      while not optimized:
        for b in np.arange(-1*(self.max_feature_value*b_range_multiple),
                           self.max_feature_value*b_range_multiple, 
                           step*b_multiple):
          for transformation in transforms:
            w_t = w*transformation 
            found_option = True
            # weakest link in the SVM fundamentally
            #SMO attempts to fix this a bit 
            # yi(xi*w + b) >= 1
            for i in self.data:
              for xi in self.data[i]:
                yi = i
                if not yi*(np.dot(w_t,xi)+b) >= 1:
                  found_option = False

            if found_option:
              opt_dict[np.linalg.norm(w_t)] = [w_t, b]   

          if w[0] < 0:
            optimized = True
            print('Optimized a step.')
          else:
            w = w - step

      norms = sorted([n for n in opt_dict])

      opt_choice = opt_dict[norms [0]]
      #||w|| : [w,b]
      self.w = opt_choice[0]
      self.b = opt_choice[1]
      latest_optimum = opt_choice[0][0]+step


  def predict(self, features):
    #sign(x*w+b)
    classification  = np.sign(np.dot(np.array(features), self.w)+self.b)   
    if classification !=0 and self.visualization:
      self.ax.scatter(features[0], features[1], s=200, marker = '*',
                      c=self.colors[classification])
      

    return classification

  def visualize(self):
    [[self.ax.scatter(x[0], x[1], s=100, color=self.colors[i]) for x in data_dict[i]] for i in data_dict]
   
    # hyperplane = x.w+b
    # v = x.w_b
    # psv = 1
    # nsv + -1
    # dec = 0
    # this is all matplotlib stuff does not matter to the svm but to humans
    def hyperplane(x,w,b,v):
      return (-w[0]*x-b+v) /  w[1]

    datarange = (self.min_feature_value*0.9, self.max_feature_value*1.1)  
    hyp_x_min = datarange[0]
    hyp_x_max = datarange[1]

    # (w.x+b) = 1
    # positive support vector hyperplane
    psv1 = hyperplane(hyp_x_min, self.w, self.b, 1)
    psv2 = hyperplane(hyp_x_max, self.w, self.b, 1)
    self.ax.plot([hyp_x_min, hyp_x_max], [psv1, psv2])

    # (w.x+b) = -1
    # negative support vector hyperplane
    nsv1 = hyperplane(hyp_x_min, self.w, self.b, -1)
    nsv2 = hyperplane(hyp_x_max, self.w, self.b, -1)
    self.ax.plot([hyp_x_min, hyp_x_max], [nsv1, nsv2])

    # (w.x+b) = 0
    # decision boundary
    db1 = hyperplane(hyp_x_min, self.w, self.b, 0)
    db2 = hyperplane(hyp_x_max, self.w, self.b, 0)
    self.ax.plot([hyp_x_min, hyp_x_max], [db1, db2])

    plt.show()

data_dict = {-1:np.array([[1, 7],
                          [2, 8],
                          [3, 8],]),
             
               1:np.array([[5, 1],
                           [6, -1],
                           [7, 3],]) }

svm = Support_Vector_Machine()
svm.fit(data=data_dict)
svm.visualize()                           
#TIMESTAMP 14:00 Are positive and negative support vectors correct?

