# -*- coding: utf-8 -*-
"""TitanicProblem_isabel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dj80jVQalNXamtpRqQICRFofLe6x-Ehb
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB 
from sklearn.linear_model import Perceptron, LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from sklearn.neighbors import NearestCentroid
from sklearn.multioutput import MultiOutputClassifier 
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn import svm
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
import warnings
warnings.filterwarnings("ignore")

train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

train_test_data = [train, test]

#train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], 1, inplace = True)
#test.drop(['Name', 'Ticket', 'Cabin'], 1, inplace = True)

train.drop(['PassengerId', 'Ticket', 'Cabin'], 1, inplace = True)
test.drop(['Ticket', 'Cabin'], 1, inplace = True)

#cabin removed because of too many missing values, although placement on
#boat potentially could affect chances of reaching means of survival
#PassengerId for Test dropped later on 

#filling in age gaps in train
for dataset in train_test_data:
    age_avg = dataset['Age'].mean()
    age_std = dataset['Age'].std()
    age_null_count = dataset['Age'].isnull().sum()
    
    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)
    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list
    dataset['Age'] = dataset['Age'].astype(int)
    
    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)

for dataset in train_test_data:    
    #Discrete variables
    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1

    dataset['IsAlone'] = 1 #initialize to yes/1 is alone
    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1

    dataset['Title'] = dataset['Name'].str.split(", ", expand=True)[1].str.split(".", expand=True)[0]

    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)

    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)

stat_min = 10 
title_names = (train['Title'].value_counts() < stat_min) #this will create a true false series with title name as index

train['Title'] = train['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)

def handle_non_numerical_data(train):
  columns = train.columns.values

  for column in columns:
    text_digit_vals = {}
    def convert_to_int(val):
      return text_digit_vals[val]

    if train[column].dtype != np.int64 and train[column].dtype != np.float64:
      column_contents = train[column].values.tolist()
      unique_elements = set(column_contents)
      x=0
      for unique in unique_elements:
        if unique not in text_digit_vals:
          text_digit_vals[unique]=x
          x+=1

      train[column] = list(map(convert_to_int, train[column]))

  return train

#converting non numerical data to numbers in test

def handle_non_numerical_data(test):
  columns = test.columns.values

  for column in columns:
    text_digit_vals = {}
    def convert_to_int(val):
      return text_digit_vals[val]

    if test[column].dtype != np.int64 and test[column].dtype != np.float64:
      column_contents = test[column].values.tolist()
      unique_elements = set(column_contents)
      x=0
      for unique in unique_elements:
        if unique not in text_digit_vals:
          text_digit_vals[unique]=x
          x+=1

      test[column] = list(map(convert_to_int, test[column]))

  return test


train = handle_non_numerical_data(train)
train = train.drop(['Name', 'Age', 'Fare'], axis = 1)
test = handle_non_numerical_data(test)
test = test.drop(['Name', 'Age', 'Fare'], axis = 1)


X_train = train.drop('Survived', axis=1)
y_train = train['Survived']
X_test = test.drop("PassengerId", axis=1).copy()
X_train, X_val, y_train, y_val = train_test_split( X_train, y_train, test_size=0.2, random_state=42)
X_train.shape, y_train.shape, X_test.shape

###############Random Forrest####################

clf_random_forest = RandomForestClassifier(n_estimators=80)
clf_random_forest.fit(X_train, y_train)

y_pred_random_forest = clf_random_forest.predict(X_val)

# Accuracy 
def accuracy(y_val,y_pred_val):
    acc = round(accuracy_score(y_val, y_pred_val) *100, 2)
    print("Validation accuracy:" + str(acc) + '%')

# RECALL
def recall(y_val,y_pred_val):
    recall = round(recall_score(y_val, y_pred_val) *100, 2)
    print("Validation Recall:" + str(recall) + '%')
# PRECISION 

def precision(y_val,y_pred_val):
  precision = round(precision_score(y_val, y_pred_val) * 100, 2)
  print("Validation Precision:" + str(precision) + '%')

# F1
def f1(y_val, y_pred_val):
  f1 = round(f1_score(y_val, y_pred_val) *100, 2)
  print("F1:" + str(f1) + "%")


accuracy(y_val,y_pred_random_forest)
recall(y_val,y_pred_random_forest)
precision(y_val,y_pred_random_forest)
f1(y_val, y_pred_random_forest)

"""# Different Classifiers"""

##### Decision Tree Classifier #####

clf_DecisionTree = DecisionTreeClassifier()
clf_DecisionTree.fit(X_train, y_train)
y_pred_decision_tree = clf_DecisionTree.predict(X_val)

accuracy(y_val, y_pred_decision_tree)
recall(y_val,y_pred_decision_tree)
precision(y_val,y_pred_decision_tree)
f1(y_val, y_pred_decision_tree)

#### Gaussian NB #####

clf_GNB = GaussianNB()
clf_GNB.fit(X_train, y_train)
y_pred_gnb = clf_GNB.predict(X_val)

accuracy(y_val,y_pred_gnb)
recall(y_val,y_pred_gnb)
precision(y_val,y_pred_gnb)
f1(y_val, y_pred_gnb)

#### Multinomial NB #####

clf_MNB = MultinomialNB()
clf_MNB.fit(X_train, y_train)
y_pred_mnb = clf_MNB.predict(X_val)

accuracy(y_val, y_pred_mnb)
recall(y_val,y_pred_mnb)
precision(y_val,y_pred_mnb)
f1(y_val, y_pred_mnb)

#### ComplementNB #####

clf_CNB = ComplementNB()
clf_CNB.fit(X_train, y_train)
y_pred_cnb = clf_CNB.predict(X_val)

accuracy(y_val, y_pred_cnb)
recall(y_val,y_pred_cnb)
precision(y_val,y_pred_cnb)
f1(y_val, y_pred_cnb)

#### BernoulliNB #####

clf_BNB = BernoulliNB()
clf_BNB.fit(X_train, y_train)
y_pred_bnb = clf_BNB.predict(X_val)

accuracy(y_val, y_pred_bnb)
recall(y_val,y_pred_bnb)
precision(y_val,y_pred_bnb)
f1(y_val, y_pred_bnb)

#### CategoricalNB #####

clf_CatNB = CategoricalNB()
clf_CatNB.fit(X_train, y_train)
y_pred_catnb = clf_CatNB.predict(X_val)

accuracy(y_val, y_pred_catnb)
recall(y_val,y_pred_catnb)
precision(y_val,y_pred_catnb)
f1(y_val, y_pred_catnb)

"""# SGD"""

# SGD 
clf_SGD = SGDClassifier()
clf_SGD.fit(X_train, y_train)
y_pred_sgc = clf_SGD.predict(X_val)

accuracy(y_val, y_pred_sgc)
recall(y_val,y_pred_sgc)
precision(y_val,y_pred_sgc)

"""# Nearest Centroid"""

clf_NC = NearestCentroid()
clf_NC.fit(X_train, y_train)
y_pred_nc = clf_NC.predict(X_val)

accuracy(y_val,y_pred_nc)
recall(y_val,y_pred_nc)
precision(y_val,y_pred_nc)
f1(y_val, y_pred_nc)

"""# SVM"""

clf_SVM = svm.SVC()
clf_SVM.fit(X_train, y_train)
y_pred_svm = clf_SVM.predict(X_val)

accuracy(y_val,y_pred_svm)
recall(y_val,y_pred_svm)
precision(y_val,y_pred_svm)
f1(y_val, y_pred_svm)

"""# Voting Classifier"""

clf1 = LogisticRegression(random_state=1)
clf2 = RandomForestClassifier(n_estimators=80)
clf3 = GaussianNB()

clf_Voting = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
clf_Voting.fit(X_train, y_train)
y_pred_svm = clf_Voting.predict(X_val)

accuracy(y_val,y_pred_svm)
recall(y_val,y_pred_svm)
precision(y_val,y_pred_svm)
f1(y_val, y_pred_svm)

"""# Neural Network"""

clf_NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,), random_state=1)
clf_NN.fit(X_train, y_train)
y_pred_nn = clf_NN.predict(X_val)

accuracy(y_val,y_pred_svm)
recall(y_val,y_pred_svm)
precision(y_val,y_pred_svm)
f1(y_val, y_pred_svm)

"""# Create Submission File"""

y_pred_test = clf_Voting.predict(X_test)

submission = pd.DataFrame({
        "PassengerId": test["PassengerId"],
        "Survived": y_pred_test
    })

submission.to_csv('titanic_submission_voting.csv', index=False)

#0.7344 score 

#0.72727 score

#0.7488

#0.78229 with voting regression